services:
  diagram2table:
    build:
      context: ..
      dockerfile: docker/Dockerfile
      target: production
      args:
        - MODEL_NAME=qwen2.5-vl-3b  # Updated to Qwen 2.5-VL 3B
    image: diagram2table:latest
    container_name: diagram2table
    
    ports:
      - "7860:7860"  # Gradio UI
      - "8000:8000"  # FastAPI
    
    volumes:
      # Persist models (download once)
      - ../models:/app/models
      # Hugging Face cache - for user 'app' (not root!)
      - hf_cache:/home/app/.cache/huggingface
      # Persist data and results
      - ../data:/app/data
      # Logs
      - ../logs:/app/logs
    
    environment:
      - DEPLOYMENT_MODE=mock
      - MODEL_NAME=mock-vlm
      - LOG_LEVEL=INFO
      - GRADIO_SHARE=false
      - HF_HOME=/home/app/.cache/huggingface
    
    # Resource limits
    deploy:
      resources:
        limits:
          memory: 8G
        reservations:
          memory: 4G
    
    restart: unless-stopped
    
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/v1/health/live"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # GPU-enabled service with Qwen2.5-VL-3B (4-bit quantized, CUDA 13.0)
  diagram2table-gpu:
    extends:
      service: diagram2table
    container_name: diagram2table-gpu
    build:
      context: ..
      dockerfile: docker/Dockerfile
      target: production
      args:
        - MODEL_NAME=qwen2.5-vl-3b  # Updated to 3B model with 4-bit support
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    environment:
      - DEPLOYMENT_MODE=vlm_quantized
      - MODEL_NAME=Qwen/Qwen2.5-VL-3B-Instruct #TODO: Возможно надо поменять, если это напрямую хардкодит выбор модели. для MVP норм
      - LOG_LEVEL=INFO
      - GRADIO_SHARE=false
      - CUDA_VISIBLE_DEVICES=0
      - HF_HOME=/home/app/.cache/huggingface
      - PYTHONUNBUFFERED=1
    # Increase timeouts for model loading on GPU
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/v1/health/ready"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 180s  # 3 minutes for model load on GPU with 4-bit quantization

volumes:
  hf_cache:
